{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressió Logística\n",
    "\n",
    "**Nom**: \n",
    "\n",
    "La **regressió logística** és un tipus d'algorisme de classificació binària (vol predir un valor $0$ o $1$ per una determinada mostra) i per fer-ho fa servir la funció logística:\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+ e^{-x}}  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFdCAYAAACJlf6EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+vxw4seS3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105c66290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "x = np.linspace(-10,10,200)\n",
    "y = [logistic(_) for _ in x]\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-0.1,1.1])\n",
    "plt.plot(x,y,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si disposem d'un conjunt de punts $\\{(x_i, y_i)\\}$, on $y_i \\in \\{0,1\\}$, i volem trobar quina és la probabilitat d'una mostra de ser $0$ o $1$ podem aproximar la probabilitat d'aquesta manera:\n",
    "\n",
    "$$ \\hat{y}_i = \\sigma(w x_i) $$\n",
    "\n",
    "on $w$ són una sèrie de pesos que cal determinar. En el cas de la regressió logísitica, la funció de cost que MAXIMITZAREM és la funció que s'anomena *log likelihood*:\n",
    "\n",
    "$$ logL = y_i \\log \\sigma(w x_i) + (1- y_i) \\log (1 - \\sigma(w x_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def logistic_log_likelihood_i(x_i, y_i, beta):\n",
    "    if y_i == 1:\n",
    "        return math.log(logistic(dot(x_i, beta)))\n",
    "    else:\n",
    "        return math.log(1 - logistic(dot(x_i, beta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per tant, la funció de cost és:\n",
    "\n",
    "$$ \\sum_{x_i, y_i} y_i \\log \\sigma(w x_i) + (1- y_i) \\log (1 - \\sigma(w x_i)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_log_likelihood(x, y, beta):\n",
    "    return sum(logistic_log_likelihood_i(x_i, y_i, beta)\n",
    "               for x_i, y_i in zip(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Les derivades d'aquestes funcions són simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_log_partial_ij(x_i, y_i, beta, j):\n",
    "    \"\"\"j es l'index de la derivada\"\"\"\n",
    "    return (y_i - logistic(dot(x_i, beta))) * x_i[j]\n",
    "    \n",
    "def logistic_log_gradient_i(x_i, y_i, beta):\n",
    "    \"\"\"gradient de log likelihood pel punt i\"\"\"\n",
    "    return [logistic_log_partial_ij(x_i, y_i, beta, j)\n",
    "            for j, _ in enumerate(beta)]\n",
    "            \n",
    "def logistic_log_gradient(x, y, beta):\n",
    "    return reduce(vector_add,\n",
    "                  [logistic_log_gradient_i(x_i, y_i, beta)\n",
    "                   for x_i, y_i in zip(x,y)])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amb això, i algunes funcions auxililars, tenim tots els ingredients per plantejar el problema com un problema d'optimització."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# funcions auxiliars\n",
    "from functools import partial\n",
    "\n",
    "def split_data(data, prob):\n",
    "    results = [], []\n",
    "    for row in data:\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    return results\n",
    "\n",
    "def train_test_split(x, y, test_pct):\n",
    "    data = zip(x, y)                                \n",
    "    train, test = split_data(data, 1 - test_pct)  \n",
    "    x_train, y_train = zip(*train)                \n",
    "    x_test, y_test = zip(*test)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                          negate_all(gradient_fn),\n",
    "                          theta_0, \n",
    "                          tolerance)\n",
    "\n",
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    \n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    \n",
    "    theta = theta_0                           # set theta to initial value\n",
    "    target_fn = safe(target_fn)               # safe version of target_fn\n",
    "    value = target_fn(theta)                  # value we're minimizing\n",
    "    \n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)  \n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                       for step_size in step_sizes]\n",
    "                   \n",
    "        # choose the one that minimizes the error function        \n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "        \n",
    "        # stop if we're \"converging\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value\n",
    "            \n",
    "def rescale(data_matrix):\n",
    "    means, stdevs = scale(data_matrix)\n",
    "\n",
    "    def rescaled(i, j): \n",
    "        if stdevs[j] > 0:\n",
    "            return (data_matrix[i][j] - means[j]) / stdevs[j]\n",
    "        else:\n",
    "            return data_matrix[i][j]\n",
    "\n",
    "    num_rows, num_cols = shape(data_matrix)\n",
    "    return make_matrix(num_rows, num_cols, rescaled)\n",
    "\n",
    "def scale(data_matrix):\n",
    "    num_rows, num_cols = shape(data_matrix)\n",
    "    means = [mean(get_column(data_matrix,j))\n",
    "             for j in range(num_cols)]\n",
    "    stdevs = [standard_deviation(get_column(data_matrix,j))\n",
    "              for j in range(num_cols)]\n",
    "    return means, stdevs\n",
    "\n",
    "def shape(A):\n",
    "    num_rows = len(A)\n",
    "    num_cols = len(A[0]) if A else 0\n",
    "    return num_rows, num_cols\n",
    "\n",
    "def mean(x): \n",
    "    return sum(x) / len(x)\n",
    "\n",
    "def get_column(A, j):\n",
    "    return [A_i[j] for A_i in A]\n",
    "\n",
    "def variance(x):\n",
    "    n = len(x)\n",
    "    deviations = de_mean(x)\n",
    "    return sum_of_squares(deviations) / (n - 1)\n",
    "    \n",
    "def standard_deviation(x):\n",
    "    return math.sqrt(variance(x))\n",
    "\n",
    "def de_mean(x):\n",
    "    x_bar = mean(x)\n",
    "    return [x_i - x_bar for x_i in x]\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def make_matrix(num_rows, num_cols, entry_fn):\n",
    "    \"\"\"returns a num_rows x num_cols matrix \n",
    "    whose (i,j)-th entry is entry_fn(i, j)\"\"\"\n",
    "    return [[entry_fn(i, j) for j in range(num_cols)]\n",
    "            for i in range(num_rows)]  \n",
    "\n",
    "def entry_fn(i, j): return A[i][j] + B[i][j]\n",
    "\n",
    "def negate(f):\n",
    "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def safe(f):\n",
    "    \"\"\"define a new function that wraps f and return it\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')         # this means \"infinity\" in Python\n",
    "    return safe_f\n",
    "\n",
    "def vector_add(v, w):\n",
    "    \"\"\"adds two vectors componentwise\"\"\"\n",
    "    return [v_i + w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def step(v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return [v_i + step_size * direction_i\n",
    "            for v_i, direction_i in zip(v, direction)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les dades que farem servir corresponen a una base de dades de 200 usuaris i contenen els seus anys d'experiència, el seu salari i el resultat d'una avaluació interna ($0$ o $1$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0.7, 48000], [1, 1.9, 48000]] [1, 0]\n"
     ]
    }
   ],
   "source": [
    "data = [(0.7,48000,1),(1.9,48000,0),(2.5,60000,1),(4.2,63000,0),(6,76000,0),(6.5,69000,0),(7.5,76000,0), \\\n",
    "        (8.1,88000,0),(8.7,83000,1),(10,83000,1),(0.8,43000,0),(1.8,60000,0),(10,79000,1),(6.1,76000,0), \\\n",
    "        (1.4,50000,0),(9.1,92000,0),(5.8,75000,0),(5.2,69000,0),(1,56000,0),(6,67000,0),(4.9,74000,0),   \\\n",
    "        (6.4,63000,1),(6.2,82000,0),(3.3,58000,0),(9.3,90000,1),(5.5,57000,1),(9.1,102000,0),(2.4,54000,0),\\\n",
    "        (8.2,65000,1),(5.3,82000,0),(9.8,107000,0),(1.8,64000,0),(0.6,46000,1),(0.8,48000,0),(8.6,84000,1),\\\n",
    "        (0.6,45000,0),(0.5,30000,1),(7.3,89000,0),(2.5,48000,1),(5.6,76000,0),(7.4,77000,0),(2.7,56000,0),\\\n",
    "        (0.7,48000,0),(1.2,42000,0),(0.2,32000,1),(4.7,56000,1),(2.8,44000,1),(7.6,78000,0),(1.1,63000,0),\\\n",
    "        (8,79000,1),(2.7,56000,0),(6,52000,1),(4.6,56000,0),(2.5,51000,0),(5.7,71000,0),(2.9,65000,0), \\\n",
    "        (1.1,33000,1),(3,62000,0),(4,71000,0),(2.4,61000,0),(7.5,75000,0),(9.7,81000,1),(3.2,62000,0),\\\n",
    "        (7.9,88000,0),(4.7,44000,1),(2.5,55000,0),(1.6,41000,0),(6.7,64000,1),(6.9,66000,1),(7.9,78000,1),\\\n",
    "        (8.1,102000,0),(5.3,48000,1),(8.5,66000,1),(0.2,56000,0),(6,69000,0),(7.5,77000,0),(8,86000,0),\\\n",
    "        (4.4,68000,0),(4.9,75000,0),(1.5,60000,0),(2.2,50000,0),(3.4,49000,1),(4.2,70000,0),(7.7,98000,0),\\\n",
    "        (8.2,85000,0),(5.4,88000,0),(0.1,46000,0),(1.5,37000,0),(6.3,86000,0),(3.7,57000,0),(8.4,85000,0),\\\n",
    "        (2,42000,0),(5.8,69000,1),(2.7,64000,0),(3.1,63000,0),(1.9,48000,0),(10,72000,1),(0.2,45000,0),\\\n",
    "        (8.6,95000,0),(1.5,64000,0),(9.8,95000,0),(5.3,65000,0),(7.5,80000,0),(9.9,91000,0),(9.7,50000,1),\\\n",
    "        (2.8,68000,0),(3.6,58000,0),(3.9,74000,0),(4.4,76000,0),(2.5,49000,0),(7.2,81000,0),(5.2,60000,1),\\\n",
    "        (2.4,62000,0),(8.9,94000,0),(2.4,63000,0),(6.8,69000,1),(6.5,77000,0),(7,86000,0),(9.4,94000,0),\\\n",
    "        (7.8,72000,1),(0.2,53000,0),(10,97000,0),(5.5,65000,0),(7.7,71000,1),(8.1,66000,1),(9.8,91000,0),\\\n",
    "        (8,84000,0),(2.7,55000,0),(2.8,62000,0),(9.4,79000,0),(2.5,57000,0),(7.4,70000,1),(2.1,47000,0),\\\n",
    "        (5.3,62000,1),(6.3,79000,0),(6.8,58000,1),(5.7,80000,0),(2.2,61000,0),(4.8,62000,0),(3.7,64000,0),\\\n",
    "        (4.1,85000,0),(2.3,51000,0),(3.5,58000,0),(0.9,43000,0),(0.9,54000,0),(4.5,74000,0),(6.5,55000,1),\\\n",
    "        (4.1,41000,1),(7.1,73000,0),(1.1,66000,0),(9.1,81000,1),(8,69000,1),(7.3,72000,1),(3.3,50000,0),\\\n",
    "        (3.9,58000,0),(2.6,49000,0),(1.6,78000,0),(0.7,56000,0),(2.1,36000,1),(7.5,90000,0),(4.8,59000,1),\\\n",
    "        (8.9,95000,0),(6.2,72000,0),(6.3,63000,0),(9.1,100000,0),(7.3,61000,1),(5.6,74000,0),(0.5,66000,0),\\\n",
    "        (1.1,59000,0),(5.1,61000,0),(6.2,70000,0),(6.6,56000,1),(6.3,76000,0),(6.5,78000,0),(5.1,59000,0),\\\n",
    "        (9.5,74000,1),(4.5,64000,0),(2,54000,0),(1,52000,0),(4,69000,0),(6.5,76000,0),(3,60000,0),(4.5,63000,0),\\\n",
    "        (7.8,70000,0),(3.9,60000,1),(0.8,51000,0),(4.2,78000,0),(1.1,54000,0),(6.2,60000,0),(2.9,59000,0),\\\n",
    "        (2.1,52000,0),(8.2,87000,0),(4.8,73000,0),(2.2,42000,1),(9.1,98000,0),(6.5,84000,0),(6.9,73000,0),\\\n",
    "        (5.1,72000,0),(9.1,69000,1),(9.8,79000,1),]\n",
    "data = map(list, data)              # canviem de tuples a llistes\n",
    "x = [[1] + row[:2] for row in data] # cada element es [1, experiencia, salari]\n",
    "y = [row[2] for row in data]        # cada element es resultat\n",
    "print x[0:2], y[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El fet de posar una de les components de $x_i$ a $1$ ens permet que el model lineal que usem pugui ser del tipus $w_0 + w_1 x_1 + w_2 x_2$, atès que $x_0 = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_hat [-1.9061824826642335, 4.053083869380028, -3.878895361783912]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "rescaled_x = rescale(x)\n",
    "x_train, x_test, y_train, y_test = train_test_split(rescaled_x, y, 0.33)\n",
    "\n",
    "# want to maximize log likelihood on the training data\n",
    "fn = partial(logistic_log_likelihood, x_train, y_train)\n",
    "gradient_fn = partial(logistic_log_gradient, x_train, y_train)\n",
    "\n",
    "# pick a random starting point\n",
    "w_0 = [1, 1, 1]\n",
    "\n",
    "# and maximize using gradient descent\n",
    "w_hat = maximize_batch(fn, gradient_fn, w_0)\n",
    "\n",
    "print \"w_hat\", w_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercici 1\n",
    "\n",
    "Avalua el resultat anterior en termes de 'precision' i 'recall'. Consulteu https://en.wikipedia.org/wiki/Precision_and_recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solució"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercici 2\n",
    "\n",
    "Feu un gràfic en que l'eix $x$ representi el valor de les prediccions i el $y$ els valors reals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Solució"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercici 3\n",
    "\n",
    "El cas anterior surt d'un nombre random fix (random.seed(0)) i d'un paràmetre fix ([1,1,1]). Feu una cerca aleatoria a veure si es pot trobar un `w_hat` sensiblement millor al [-1.9061824826642335, 4.053083869380028, -3.878895361783912]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solució"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
