{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressió Logística\n",
    "\n",
    "**Nom**: \n",
    "\n",
    "La **regressió logística** és un tipus d'algorisme de classificació binària (vol predir un valor $0$ o $1$ per una determinada mostra) i per fer-ho fa servir la funció logística:\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+ e^{-x}}  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "x = np.linspace(-10,10,200)\n",
    "y = [logistic(_) for _ in x]\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-0.1,1.1])\n",
    "plt.plot(x,y,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si disposem d'un conjunt de punts $\\{(x_i, y_i)\\}$, on $y_i \\in \\{0,1\\}$, i volem trobar quina és la probabilitat d'una mostra de ser $0$ o $1$ podem aproximar la probabilitat d'aquesta manera:\n",
    "\n",
    "$$ \\hat{y}_i = \\sigma(w x_i) $$\n",
    "\n",
    "on $w$ són una sèrie de pesos que cal determinar. En el cas de la regressió logísitica, la funció de cost que MAXIMITZAREM és la funció que s'anomena *log likelihood*:\n",
    "\n",
    "$$ logL = y_i \\log \\sigma(w x_i) + (1- y_i) \\log (1 - \\sigma(w x_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def logistic_log_likelihood_i(x_i, y_i, beta):\n",
    "    if y_i == 1:\n",
    "        return math.log(logistic(dot(x_i, beta)))\n",
    "    else:\n",
    "        return math.log(1 - logistic(dot(x_i, beta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per tant, la funció de cost és:\n",
    "\n",
    "$$ \\sum_{x_i, y_i} y_i \\log \\sigma(w x_i) + (1- y_i) \\log (1 - \\sigma(w x_i)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_log_likelihood(x, y, beta):\n",
    "    return sum(logistic_log_likelihood_i(x_i, y_i, beta)\n",
    "               for x_i, y_i in zip(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Les derivades d'aquestes funcions són simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_log_partial_ij(x_i, y_i, beta, j):\n",
    "    \"\"\"j es l'index de la derivada\"\"\"\n",
    "    return (y_i - logistic(dot(x_i, beta))) * x_i[j]\n",
    "    \n",
    "def logistic_log_gradient_i(x_i, y_i, beta):\n",
    "    \"\"\"gradient de log likelihood pel punt i\"\"\"\n",
    "    return [logistic_log_partial_ij(x_i, y_i, beta, j)\n",
    "            for j, _ in enumerate(beta)]\n",
    "            \n",
    "def logistic_log_gradient(x, y, beta):\n",
    "    return reduce(vector_add,\n",
    "                  [logistic_log_gradient_i(x_i, y_i, beta)\n",
    "                   for x_i, y_i in zip(x,y)])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amb això, i algunes funcions auxililars, tenim tots els ingredients per plantejar el problema com un problema d'optimització."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# funcions auxiliars\n",
    "from functools import partial\n",
    "\n",
    "def split_data(data, prob):\n",
    "    results = [], []\n",
    "    for row in data:\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    return results\n",
    "\n",
    "def train_test_split(x, y, test_pct):\n",
    "    data = zip(x, y)                                \n",
    "    train, test = split_data(data, 1 - test_pct)  \n",
    "    x_train, y_train = zip(*train)                \n",
    "    x_test, y_test = zip(*test)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                          negate_all(gradient_fn),\n",
    "                          theta_0, \n",
    "                          tolerance)\n",
    "\n",
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    \n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    \n",
    "    theta = theta_0                           # set theta to initial value\n",
    "    target_fn = safe(target_fn)               # safe version of target_fn\n",
    "    value = target_fn(theta)                  # value we're minimizing\n",
    "    \n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)  \n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                       for step_size in step_sizes]\n",
    "                   \n",
    "        # choose the one that minimizes the error function        \n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "        \n",
    "        # stop if we're \"converging\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value\n",
    "            \n",
    "def rescale(data_matrix):\n",
    "    means, stdevs = scale(data_matrix)\n",
    "\n",
    "    def rescaled(i, j): \n",
    "        if stdevs[j] > 0:\n",
    "            return (data_matrix[i][j] - means[j]) / stdevs[j]\n",
    "        else:\n",
    "            return data_matrix[i][j]\n",
    "\n",
    "    num_rows, num_cols = shape(data_matrix)\n",
    "    return make_matrix(num_rows, num_cols, rescaled)\n",
    "\n",
    "def scale(data_matrix):\n",
    "    num_rows, num_cols = shape(data_matrix)\n",
    "    means = [mean(get_column(data_matrix,j))\n",
    "             for j in range(num_cols)]\n",
    "    stdevs = [standard_deviation(get_column(data_matrix,j))\n",
    "              for j in range(num_cols)]\n",
    "    return means, stdevs\n",
    "\n",
    "def shape(A):\n",
    "    num_rows = len(A)\n",
    "    num_cols = len(A[0]) if A else 0\n",
    "    return num_rows, num_cols\n",
    "\n",
    "def mean(x): \n",
    "    return sum(x) / len(x)\n",
    "\n",
    "def get_column(A, j):\n",
    "    return [A_i[j] for A_i in A]\n",
    "\n",
    "def variance(x):\n",
    "    n = len(x)\n",
    "    deviations = de_mean(x)\n",
    "    return sum_of_squares(deviations) / (n - 1)\n",
    "    \n",
    "def standard_deviation(x):\n",
    "    return math.sqrt(variance(x))\n",
    "\n",
    "def de_mean(x):\n",
    "    x_bar = mean(x)\n",
    "    return [x_i - x_bar for x_i in x]\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def make_matrix(num_rows, num_cols, entry_fn):\n",
    "    \"\"\"returns a num_rows x num_cols matrix \n",
    "    whose (i,j)-th entry is entry_fn(i, j)\"\"\"\n",
    "    return [[entry_fn(i, j) for j in range(num_cols)]\n",
    "            for i in range(num_rows)]  \n",
    "\n",
    "def entry_fn(i, j): return A[i][j] + B[i][j]\n",
    "\n",
    "def negate(f):\n",
    "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def safe(f):\n",
    "    \"\"\"define a new function that wraps f and return it\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')         # this means \"infinity\" in Python\n",
    "    return safe_f\n",
    "\n",
    "def vector_add(v, w):\n",
    "    \"\"\"adds two vectors componentwise\"\"\"\n",
    "    return [v_i + w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def step(v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return [v_i + step_size * direction_i\n",
    "            for v_i, direction_i in zip(v, direction)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les dades que farem servir corresponen a una base de dades de 200 usuaris i contenen els seus anys d'experiència, el seu salari i el resultat d'una avaluació interna ($0$ o $1$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [(0.7,48000,1),(1.9,48000,0),(2.5,60000,1),(4.2,63000,0),(6,76000,0),(6.5,69000,0),(7.5,76000,0), \\\n",
    "        (8.1,88000,0),(8.7,83000,1),(10,83000,1),(0.8,43000,0),(1.8,60000,0),(10,79000,1),(6.1,76000,0), \\\n",
    "        (1.4,50000,0),(9.1,92000,0),(5.8,75000,0),(5.2,69000,0),(1,56000,0),(6,67000,0),(4.9,74000,0),   \\\n",
    "        (6.4,63000,1),(6.2,82000,0),(3.3,58000,0),(9.3,90000,1),(5.5,57000,1),(9.1,102000,0),(2.4,54000,0),\\\n",
    "        (8.2,65000,1),(5.3,82000,0),(9.8,107000,0),(1.8,64000,0),(0.6,46000,1),(0.8,48000,0),(8.6,84000,1),\\\n",
    "        (0.6,45000,0),(0.5,30000,1),(7.3,89000,0),(2.5,48000,1),(5.6,76000,0),(7.4,77000,0),(2.7,56000,0),\\\n",
    "        (0.7,48000,0),(1.2,42000,0),(0.2,32000,1),(4.7,56000,1),(2.8,44000,1),(7.6,78000,0),(1.1,63000,0),\\\n",
    "        (8,79000,1),(2.7,56000,0),(6,52000,1),(4.6,56000,0),(2.5,51000,0),(5.7,71000,0),(2.9,65000,0), \\\n",
    "        (1.1,33000,1),(3,62000,0),(4,71000,0),(2.4,61000,0),(7.5,75000,0),(9.7,81000,1),(3.2,62000,0),\\\n",
    "        (7.9,88000,0),(4.7,44000,1),(2.5,55000,0),(1.6,41000,0),(6.7,64000,1),(6.9,66000,1),(7.9,78000,1),\\\n",
    "        (8.1,102000,0),(5.3,48000,1),(8.5,66000,1),(0.2,56000,0),(6,69000,0),(7.5,77000,0),(8,86000,0),\\\n",
    "        (4.4,68000,0),(4.9,75000,0),(1.5,60000,0),(2.2,50000,0),(3.4,49000,1),(4.2,70000,0),(7.7,98000,0),\\\n",
    "        (8.2,85000,0),(5.4,88000,0),(0.1,46000,0),(1.5,37000,0),(6.3,86000,0),(3.7,57000,0),(8.4,85000,0),\\\n",
    "        (2,42000,0),(5.8,69000,1),(2.7,64000,0),(3.1,63000,0),(1.9,48000,0),(10,72000,1),(0.2,45000,0),\\\n",
    "        (8.6,95000,0),(1.5,64000,0),(9.8,95000,0),(5.3,65000,0),(7.5,80000,0),(9.9,91000,0),(9.7,50000,1),\\\n",
    "        (2.8,68000,0),(3.6,58000,0),(3.9,74000,0),(4.4,76000,0),(2.5,49000,0),(7.2,81000,0),(5.2,60000,1),\\\n",
    "        (2.4,62000,0),(8.9,94000,0),(2.4,63000,0),(6.8,69000,1),(6.5,77000,0),(7,86000,0),(9.4,94000,0),\\\n",
    "        (7.8,72000,1),(0.2,53000,0),(10,97000,0),(5.5,65000,0),(7.7,71000,1),(8.1,66000,1),(9.8,91000,0),\\\n",
    "        (8,84000,0),(2.7,55000,0),(2.8,62000,0),(9.4,79000,0),(2.5,57000,0),(7.4,70000,1),(2.1,47000,0),\\\n",
    "        (5.3,62000,1),(6.3,79000,0),(6.8,58000,1),(5.7,80000,0),(2.2,61000,0),(4.8,62000,0),(3.7,64000,0),\\\n",
    "        (4.1,85000,0),(2.3,51000,0),(3.5,58000,0),(0.9,43000,0),(0.9,54000,0),(4.5,74000,0),(6.5,55000,1),\\\n",
    "        (4.1,41000,1),(7.1,73000,0),(1.1,66000,0),(9.1,81000,1),(8,69000,1),(7.3,72000,1),(3.3,50000,0),\\\n",
    "        (3.9,58000,0),(2.6,49000,0),(1.6,78000,0),(0.7,56000,0),(2.1,36000,1),(7.5,90000,0),(4.8,59000,1),\\\n",
    "        (8.9,95000,0),(6.2,72000,0),(6.3,63000,0),(9.1,100000,0),(7.3,61000,1),(5.6,74000,0),(0.5,66000,0),\\\n",
    "        (1.1,59000,0),(5.1,61000,0),(6.2,70000,0),(6.6,56000,1),(6.3,76000,0),(6.5,78000,0),(5.1,59000,0),\\\n",
    "        (9.5,74000,1),(4.5,64000,0),(2,54000,0),(1,52000,0),(4,69000,0),(6.5,76000,0),(3,60000,0),(4.5,63000,0),\\\n",
    "        (7.8,70000,0),(3.9,60000,1),(0.8,51000,0),(4.2,78000,0),(1.1,54000,0),(6.2,60000,0),(2.9,59000,0),\\\n",
    "        (2.1,52000,0),(8.2,87000,0),(4.8,73000,0),(2.2,42000,1),(9.1,98000,0),(6.5,84000,0),(6.9,73000,0),\\\n",
    "        (5.1,72000,0),(9.1,69000,1),(9.8,79000,1),]\n",
    "data = map(list, data)              # canviem de tuples a llistes\n",
    "x = [[1] + row[:2] for row in data] # cada element es [1, experiencia, salari]\n",
    "y = [row[2] for row in data]        # cada element es resultat\n",
    "print x[0:2], y[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El fet de posar una de les components de $x_i$ a $1$ ens permet que el model lineal que usem pugui ser del tipus $w_0 + w_1 x_1 + w_2 x_2$, atès que $x_0 = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "rescaled_x = rescale(x)\n",
    "x_train, x_test, y_train, y_test = train_test_split(rescaled_x, y, 0.33)\n",
    "\n",
    "# want to maximize log likelihood on the training data\n",
    "fn = partial(logistic_log_likelihood, x_train, y_train)\n",
    "gradient_fn = partial(logistic_log_gradient, x_train, y_train)\n",
    "\n",
    "# pick a random starting point\n",
    "w_0 = [1, 1, 1]\n",
    "\n",
    "# and maximize using gradient descent\n",
    "w_hat = maximize_batch(fn, gradient_fn, w_0)\n",
    "\n",
    "print \"w_hat\", w_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercici 1\n",
    "\n",
    "Avalua el resultat anterior en termes de 'precision' i 'recall'. Consulteu https://en.wikipedia.org/wiki/Precision_and_recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solució\n",
    "def predict(weights, to_be_predicted):\n",
    "    probability = logistic(dot(weights, to_be_predicted))\n",
    "    if probability >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def precision_recall(x_test, y_test, weights=w_hat):\n",
    "    y_predicted = map(lambda x: predict(weights, x), x_test)\n",
    "    zipped_results = zip(y_predicted, y_test)\n",
    "    true_positives = sum(map(lambda x: x[0] == x[1], zipped_results))\n",
    "    false_positives = sum(map(lambda x: x[0] == 1 and x[1] == 0, zipped_results))\n",
    "    precision = float(true_positives) / float(true_positives + false_positives)\n",
    "    # Recall now\n",
    "    false_negatives = sum(map(lambda x: x[0] == 0 and x[1] == 1, zipped_results))\n",
    "    recall = float(true_positives) / float(true_positives + false_negatives)\n",
    "    return precision, recall\n",
    "\n",
    "result = precision_recall(x_test, y_test, w_hat)\n",
    "print \"Precision:\", 100 * result[0], '%'\n",
    "print \"Recall:\", 100 * result[1], '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercici 2\n",
    "\n",
    "Feu un gràfic en que l'eix $x$ representi el valor de les prediccions i el $y$ els valors reals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Solució\n",
    "def raw_predict(weights, to_be_predicted):\n",
    "    return logistic(dot(weights, to_be_predicted))\n",
    "\n",
    "y_predicted = map(lambda x: raw_predict(w_hat, x), x_test)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-0.1, 1.1])\n",
    "axes.set_xlim([-0.1, 1.1])\n",
    "plt.plot(y_predicted,y_test,'p')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercici 3\n",
    "\n",
    "El cas anterior surt d'un nombre random fix (random.seed(0)) i d'un paràmetre fix ([1,1,1]). Feu una cerca aleatoria a veure si es pot trobar un `w_hat` sensiblement millor al [-1.9061824826642335, 4.053083869380028, -3.878895361783912]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solució\n",
    "random.seed()\n",
    "\n",
    "# pick a random starting point\n",
    "def get_initial_weights():\n",
    "    return [(random.random() - 0.5)*20 for _ in range(3)]\n",
    "# and maximize using gradient descent\n",
    "\n",
    "def F_score_fun(beta):\n",
    "    def f(precision, recall):\n",
    "        return (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)\n",
    "    return f\n",
    "\n",
    "w_hat = [-1.9061824826642335, 4.053083869380028, -3.878895361783912]\n",
    "\n",
    "for _ in range(10):\n",
    "    w_random = get_initial_weights()\n",
    "    precision, recall = precision_recall(x_test, y_test, w_random)\n",
    "    print \"Precision:\", precision*100, \"%; Recall:\", recall*100, \"%\"\n",
    "    \n",
    "\n",
    "F_score = F_score_fun(1)\n",
    "F_score_ours = F_score(*precision_recall(x_test, y_test, weights=w_random))\n",
    "F_score_theirs = F_score(*precision_recall(x_test, y_test, weights=w_hat))\n",
    "MAX_ITER = 500\n",
    "iters = 0\n",
    "while F_score_ours <= F_score_theirs and iters < MAX_ITER:\n",
    "    w_random = get_initial_weights()\n",
    "    F_score_ours = F_score(*precision_recall(x_test, y_test, weights=w_random))\n",
    "    iters += 1\n",
    "if iters == MAX_ITER:\n",
    "    print \"No s'ha trobat un F-Score superior\"\n",
    "print \"w_hat\", w_hat\n",
    "print \"w_0\", w_0\n",
    "print \"F_scores (w_hat, w_random):\", F_score_ours, F_score_theirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
