{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctiques de Nous Usos de la Informàtica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nom de les persones del grup:** Jordi Olivares Provencio & Christian José Soler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pràctica 3: Naive Bayes i Classificació"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Què s’ha de fer?</b><br>\n",
    "Volem classificar notícies corresponents al diari New York Times segons de quin tòpic parlin principalment. A partir  de totes les notíces que tenim guardades en un fitxer .csv, crearem un vector de característiques que ens descrigui  cada notícia. Finalment desenvoluparem un classificador probabilístic del tipus Naive Bayes que ens permeti identificar a quin tòpic pertany una notícia donada segons les característiques triades.<br>\n",
    "\n",
    "<b>Quina és la idea del sistema de classificació que s’ha de desenvolupar?</b><br>\n",
    "L'objectiu del classificador és, donat un vector de característiques que descriuen els objectes que es volen classificar, indicar a quina categoria o classe pertanyen d'entre un conjunt predeterminat. El procés de classificació consta de dues parts:\n",
    "\n",
    "+ el procés d'aprenentatge \n",
    "+ el procés d'explotació o testeig. \n",
    "\n",
    "El procés d'aprenentatge rep exemples de parelles $(x,y)$ on $x$ són les característiques, usualment nombres reals, i $y$ és la categoria a la que pertanyen. Aquest conjunt se'l coneix com a conjunt d'entrenament i ens servirà per trobar una funció $y=h(x)$ que donada una $x$ m'indiqui quina és la $y$. Per altra banda el procés de testeig aplica la funció $h(x)$ apresa a l'entrenament a una nova descripció per veure quina categoria li correspon.</br>\n",
    "\n",
    "<b>Classificació i llenguatge natural</b><br>\n",
    "La descripció dels exemples en característiques és el punt més crític de tot sistema d'aprenentatge automàtic. Una de les representacions més simples per tal de descriure un text és la representació bag-of-words. Aquesta representació converteix un text en un vector de $N$ paraules. Consisteix en seleccionar un conjunt d'$N$ paraules i per cada paraula contar quants cops apareix en el text. Una versió alternativa d'aquest procés pot ser simplement indicar si apareix o no en el text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dades del New York Times</b>\n",
    "En aquest exemple, el nostre objectiu és classificar automàticament les notícies d'acord al seu titular en vint temes o tòpics. Les dades que disposem són cadascú dels articles de primera plana del New York Times entre 1996 i 2006, classificats segons Policy Agendas (http://www.policyagendas.org ). Aquesta recollida de dades l'ha compilat n'Amber E. Boydstun.\n",
    "\n",
    "Concretament, els tòpics són els següents\n",
    "\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "<td>\n",
    "1 \n",
    "<td>\n",
    "Macroeconomics\n",
    "<tr>\n",
    "<td>\n",
    "2 \n",
    "<td>\n",
    "Civil Rights, Minority Issues, and Civil Liberties \n",
    "<tr>\n",
    "<td>\n",
    "3\n",
    "<td>\n",
    "Health\n",
    "<tr>\n",
    "<td>\n",
    "4 \n",
    "<td>Agriculture\n",
    "<tr>\n",
    "<td>\n",
    "5 \n",
    "<td>Labor, Employment, and Immigration\n",
    "<tr>\n",
    "<td>\n",
    "6 \n",
    "<td> Education\n",
    "<tr>\n",
    "<td>\n",
    "7\n",
    "<td>Environment\n",
    "<tr>\n",
    "<td>\n",
    "8\n",
    "<td>Energy\n",
    "<tr>\n",
    "<td>\n",
    "10 \n",
    "<td>Transportation\n",
    "<tr>\n",
    "<td>\n",
    "12 \n",
    "<td>Law, Crime, and Family Issues\n",
    "<tr>\n",
    "<td>\n",
    "13 \n",
    "<td>Social Welfare\n",
    "<tr>\n",
    "<td>\n",
    "14 \n",
    "<td>Community Development and Housing Issues\n",
    "<tr>\n",
    "<td>\n",
    "15 \n",
    "<td>Banking, Finance, and Domestic Commerce\n",
    "<tr>\n",
    "<td>\n",
    "16 \n",
    "<td>Defense\n",
    "<tr>\n",
    "<td>\n",
    "17 \n",
    "<td>Space, Science, Technology and Communications\n",
    "<tr>\n",
    "<td>\n",
    "18 \n",
    "<td>Foreign Trade\n",
    "<tr>\n",
    "<td>\n",
    "19 \n",
    "<td>International Affairs and Foreign Aid\n",
    "<tr>\n",
    "<td>\n",
    "20 \n",
    "<td>Government Operations\n",
    "<tr>\n",
    "<td>\n",
    "21 \n",
    "<td>Public Lands and Water Management\n",
    "<tr>\n",
    "<td>\n",
    "24 \n",
    "<td>State and Local Government Administration\n",
    "<tr>\n",
    "<td>\n",
    "26 \n",
    "<td>Weather and Natural Disasters\n",
    "<tr>\n",
    "<td>\n",
    "27 \n",
    "<td>Fires\n",
    "<tr>\n",
    "<td>\n",
    "28 \n",
    "<td>Arts and Entertainment\n",
    "<tr>\n",
    "<td>\n",
    "29 \n",
    "<td>Sports and Recreation\n",
    "<tr>\n",
    "<td>\n",
    "30 \n",
    "<td>Death Notices\n",
    "<tr>\n",
    "<td>\n",
    "31 \n",
    "<td>Churches and Religion\n",
    "<tr>\n",
    "<td>\n",
    "99 \n",
    "<td>Other, Miscellaneous, and Human Interest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exemple.</b>\n",
    "Imaginem que tenim 4 notícies que pertanyen només a dues categories ``y={'8. Energy', '2. Civil Rights, Minority Issues, and Civil Liberties'}``, podríem seleccionar les següents paraules per tal de distingir les dues categories ``x={'nuclear', 'verd', 'ecologia', 'independència', 'autonomia', 'referèndum'}``.\n",
    "\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "<td></td>\n",
    "<td>nuclear</td>\n",
    "<td>verd</td>\n",
    "<td>ecologia</td>\n",
    "<td>independència</td>\n",
    "<td>autonomia</td>\n",
    "<td>referèndum</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>news 1('2. Civil Rights...')</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>2</td>\n",
    "<td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>news 2('2. Civil Rights...')</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>2</td>\n",
    "<td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>news 3('8. Energy')</td>\n",
    "<td>1</td>\n",
    "<td>3</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>news 4('8. Energy')</td>\n",
    "<td>2</td>\n",
    "<td>1</td>\n",
    "<td>2</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "amb aquesta representació la notícia 2, corresponent a la categoria ``'2. Civil Rights...'``, quedaria representat pel vector numèric ``(0,1,0,1,2,3)``. Si fem servir la representació alternativa (booleana) tindríem ``(0,1,0,1,1,1)`` que indica la presència de les paraules. Si la descripció és adient s'espera que les categories es puguin distingir entre elles amb facilitat.\n",
    "<br><br>\n",
    "<b>El classificador Naïve Bayes.</b><br>\n",
    "Un cop tenim una representació necessitem un procés d'aprenentatge que ens permeti passar de la descripció a una categoria. En aquesta pràctica farem servir el classificador Naïve Bayes. Aquest classificador forma part de la família de classificadors probabilístics. La sortida d'un classificador probabilístic és un valor de probabilitat donat un exemple per cadascuna de les categories. La decisió final correspon a la categoria amb més probabilitat. Per exemple, amb la descripció anterior esperem que la sortida sigui de l'estil,<br>\n",
    "\n",
    "$$p( y = 'Civil\\_Rights' \\; | \\; x = (0,1,0,1,1,1)) = 0.6$$\n",
    "$$p( y = 'Energy' \\; | \\; x = (0,1,0,1,1,1)) = 0.4$$\n",
    "\n",
    "<br>\n",
    "Els classificadors probabilistics Bayesians es basen en el teorema de Bayes per realitzar els càlculs per trobar la probabilitat condicionada: <br>\n",
    "\n",
    "$$ p(x,y) = p(x|y)p(y) = p(y|x)p(x)$$\n",
    "<br>\n",
    "d'on podem extreure que: <br>\n",
    "$$ p(y,x) = \\frac{p(x|y)p(y)}{p(x)}$$\n",
    "<br>\n",
    "\n",
    "\n",
    "En molts casos p(y) i p(x) són desconeguts i es consideren equiprobables. Per tant, la\n",
    "decisió es simplifica a:\n",
    "<br>\n",
    "$$ p(y|x) = c · p(x|y)$$\n",
    "\n",
    "<br>\n",
    "Les deduccions fins a aquest punt són vàlides per la majoria de classificadors Bayesians. Naïve Bayes es distingeix de la resta perquè imposa una condició encara més restrictiva. Considerem $x=(x_1,x_2,x_3,...,x_N)$ un conjunt d'$N$ variables aleatòries. Naïve Bayes assumeix que totes elles són independents entre elles i per tant podem escriure:\n",
    "<br>\n",
    "$$p(x_1,x_2,...,x_N | y) = p(x_1|y)p(x_2|y)...p(x_N|y)$$\n",
    "\n",
    "<br>\n",
    "Per tant en el nostre cas es pot veure com:\n",
    "<br>\n",
    "$$p(y='Civil\\_Rights' \\; |\\;x=(0,1,0,1,1,1)) = p(x_1=0|\\;'Civil\\_Rights' \\; )p(x_2=1|\\;'Civil\\_Rights' \\; )...p(x_6=1|\\;'Civil\\_Rights' \\; )$$\n",
    "\n",
    "<br>\n",
    "Podem interpretar l'anterior equació de la següent forma: La probabilitat de que el document descrit pel vector de característiques (0,1,0,1,1,1) sigui de la classe \"Civil Rights\" és proporcional al producte de la probabilitat que la primera paraula del vector (nuclear) no aparegui en les notícies sobre \"Civil Rights\"  per la probabilitat que la segona paraula sí que hi aparegui, etc.\n",
    "\n",
    "<br>\n",
    "<b>Estimant les probabilitats marginals condicionades</b>\n",
    "L'últim pas que ens queda és trobar el valor de les probabilitats condicionades. Farem servir la representació de 0's i 1's indicant que la paraula no apareix (0) o sí apareix (1) a la notícia. Per trobar el valor de la probabilitat condicionada farem servir una aproximació freqüentista a la probabilitat. Això vol dir que calcularem la freqüència d'aparició de cada paraula per a cada categoria. Aquest càlcul es fa dividint el nombre de notícies de la categoria en que apareix la paraula pel nombre total de notícies d'aquella categoria. En l'exemple anterior, \n",
    "$p('verd'=1 |y='Civil\\_Rights')=1/2 $, mentre que  $p('verd' =1 |y='Energy')=2/2 $\n",
    "\n",
    "En general:\n",
    "<br>\n",
    "$$p(x = 1 | y = C)= \\frac{A}{B} $$\n",
    "<br>\n",
    "on A és el número de notícies de la categoria C on hi apareix la paraula 'x' i B és el número total de notícies de la categoria C.\n",
    "\n",
    "\n",
    "<b>Punts dèbils: </b><br>\n",
    "\n",
    "<b> * El problema de la probabilitat 0</b>\n",
    "Si us hi fixeu bé, en l'anterior exemple la probabilitat <b>p('verd'= 0 | y='Energy')</b> és 0 !! Això vol dir, que si en la notícia no hi apareix la paraula 'verd' no pot ser classificada com a categoria 'Energy'!! No sembla raonable que s'assigni o no en aquesta categoria segons si en la notícia hi apareix o no una única paraula. Per tant, el que s'acostuma a fer és donar una baixa probabilitat en comptes de zero. Una de les possibles solucions es fer servir la correcció de Laplace. Seguint l'exemple anterior la correcció de Laplace és\n",
    "\n",
    "<br>\n",
    "$$p(x=1 | y = 'C' ) = \\frac{A+1}{B+M}$$ \n",
    "on M és el nombre de catergories\n",
    "\n",
    "<b> * El problema de com escollir el vector de característiques</b>\n",
    "L'elecció de les paraules que formen el vector de característiques és un pas crític. En funció de com de bona sigui aquesta descripció, millor funcionarà el sistema. Tot i que us deixem a vosaltres la política de creació del vector de característiques us donem una d'exemple. Per saber quines paraules fer servir podeu seleccionar de totes les paraules de totes les notícies aquelles que apareixen entre en un 10 i un 50 percent del total (sense tenir en compte la categoria). Podeu experimentar variant aquests valors.\n",
    "\n",
    "<b> * El problema del \"underflow\"</b>\n",
    " La funció que hem de calcular en el Naive Bayes és un producte. El nombre de caractéristiques del vector és el nombre de termes del producte. Aquests nombres són iguals o menors a 1, si els multipliquem tots entre ells el resultat serà massa petit per a representar-lo en un nombre de punt flotant i el càlcul acabarà sent reduït a zero. Per solucionar aquest problema en comptes d'operar fent multiplicacions, se sol passar a l'escala logarítmica i allà operar fent servir sumes en comptes de multiplicacions\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "#load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=pd.read_csv('Boydstun_NYT_FrontPage_Dataset_1996-2006_0.csv',index_col=\"Article_ID\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 1 </b> Escriure una funció <b>count_news(dataframe)</b> que retorni el nombre total de noticies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Retorna el número total de notícies. \n",
    "def count_news(df):\n",
    "    return len(df)\n",
    "print count_news(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 2:</b> Escriure una funció <b>count_topic_news(dataframe)</b> que compti quantes notícies hi ha per cadascun dels tópics. Agaferem el camp \"Topic_2digit\" com a identificador de tópic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Retorna una Series que compte el número de notícies per a cadascun dels tópics\n",
    "def count_topic_news(df):\n",
    "    return df.groupby('Topic_2digit').count()['Date']\n",
    "\n",
    "count_topic_news(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 3:</b> Escriure una funció <b>count_words(dataframe)</b> que retorni un diccionari amb totes les paraules que hi hagi en totes les notícies, indicant per cada paraula quantes ocurrències en total hi ha i en quantes notícies surt.\n",
    "<br>Possible format sortida: {word :  {n_ocur: valor;n_news: valor}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aquesta funció ha de construir un diccionari que contingui totes les paraules que s'han trobat indicant \n",
    "# el total de cops que ha aparegut i el nombre de notícies on apareix\n",
    "def count_words(df):        \n",
    "    word_dicc={}       \n",
    "    for c in df['Title']:\n",
    "        tmp = set()\n",
    "        for word in c.lower().split():\n",
    "            try:\n",
    "                word_dicc[word]['n_occur'] += 1\n",
    "                tmp.add(word)\n",
    "            except:\n",
    "                word_dicc[word] = {'n_occur' : 1, 'n_news' : 1}\n",
    "        for word in tmp:\n",
    "            word_dicc[word]['n_news'] += 1\n",
    "    return word_dicc\n",
    "\n",
    "\n",
    "dicc_text = count_words(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 4:</b> escriure una funció <b>count_words_topic(dataframe,dicc_text)</b> que retorna un diccionari que conté el nombre de cops que ha aparegut  cada paraula i el número de notícies on  ha aparegut. Aquesta informació ha de ser dividida pels diferents tópics de noticies.\n",
    "<br>Possible format sortida: {Topic :  {word :  {n_ocur: valor;n_news: valor} } } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compta la freqüència de les paraules per a un tòpic determinat\n",
    "def count_words_topic(df):\n",
    "    topic_dicc = {topic : {} for topic in df[\"Topic_2digit\"].unique()}\n",
    "    for _, article in df.iterrows():\n",
    "        tmp = set()\n",
    "        for word in article.loc[\"Title\"].lower().split():\n",
    "            try:\n",
    "                topic_dicc[article.loc[\"Topic_2digit\"]][word]['n_occur'] += 1\n",
    "                if word not in tmp:\n",
    "                    topic_dicc[article.loc[\"Topic_2digit\"]][word][\"n_news\"] += 1\n",
    "            except:\n",
    "                topic_dicc[article.loc[\"Topic_2digit\"]][word] = {\"n_occur\" : 1, \"n_news\" : 1}\n",
    "            tmp.add(word)\n",
    "    return topic_dicc\n",
    "\n",
    "words_topics=count_words_topic(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 5:</b> Calcular amb la funció <b>topNword(df,words_topics,N)</b> quines son les N paraules més representatives (les que apareixen amb més freqüència) de cadascun dels tòpics. Retorneu un diccionari amb els següent format: {1: llista_top_words_topic_1; 2: llista_top_words_topic_2;...}\n",
    "<br>Teniu en compte que també haureu de filtrar aquelles paraules que apareixen en la majoria de notícies, així com també, les que únicament apareixen en un conjunt molt petit de notícies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calcula les N paruales més representativa de cada tòpic . La sortida ha de \n",
    "# ser un diccionari on tenim tantes entrades com tòpics\n",
    "# el valors de les entrades ha de ser una llista amb les paraules seleccionades.\n",
    "from heapq import nlargest\n",
    "def topNwords(df,words_topics,N):\n",
    "    top_words = {topic : {} for topic in words_topics}\n",
    "    for topic in words_topics:\n",
    "        words = words_topics[topic]\n",
    "        top_words[topic] = nlargest(N, words, key=lambda x : words[x]['n_occur'])\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 6</b>: Creeu el vector de característiques necessari per a fer l’entrenament del Naïve Bayes (funció <b>create_features()</b>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Crea el vector de característiques necessari per a l'entrenament del classificador Naive Bayes\n",
    "# selected_words: ha de ser el diccionari que retorna topNWords.\n",
    "# train_data : conté totes les notícies del conjunt d'entrenament\n",
    "# Retorna un diccionari que conté un np.array per a cadascuna de les notícies amb el vector de característiques corresponent.\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "def create_features(train_data, topicWords):\n",
    "    dict_feat_vector = {news : {} for news in train_data.index}\n",
    "    for index, article in train_data.iterrows():\n",
    "        vecFeature = np.array([0] * len(topicWords))\n",
    "        a = set(article.loc[\"Title\"].lower().split())\n",
    "        for i in xrange(len(vecFeature)):\n",
    "            vecFeature[i] = 1 if topicWords[i] in a else 0\n",
    "        dict_feat_vector[index] = vecFeature\n",
    "        \n",
    "    return dict_feat_vector\n",
    "\n",
    "N = 20 # Aquest parametre el podem canviar i fer proves per avaluar quin és el millor valor. \n",
    "words_topics=count_words_topic(data)\n",
    "top_words=topNwords(data,words_topics,N)\n",
    "topic_words = list(chain.from_iterable(top_words.values()))\n",
    "dict_feat_vector = create_features(data, topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 7</b>: Implementeu la funció d'aprenentatge del classificador Naïve Bayes (funció <b>naive_bayes_learn()</b>).  La funció ha de mostrar per pantalla el resultat obtingut \n",
    "<br>\n",
    "<b> * L'error d'entrenament</b>\n",
    "L'error d'entrenament es troba calculant el percentatge d'errors que s'obtenen quan es fa el testeig amb les mateixes dades utilizades per fer entrenament (aprenentatge). Aquest error es un valor molt optimista de com funcionarà el clasificador i mai s'ha de prendre com a mesura per comparar clasificadors.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CHRISTIAN: Per totes les notícies, calcular la probabilitat que pertànyi a cada tòpic \n",
    "    # i tornar el tòpic més probable per cada notícia\n",
    "\n",
    "#Mètode que implementa el clasificador Naive_Bayes.Ha de mostrar el resultat obtingut per pantalla\n",
    "\n",
    "def naive_bayes(df, dict_feat):\n",
    "    from math import log\n",
    "    guess = {}\n",
    "    count_top = count_topic_news(df)\n",
    "    M = len(count_top)\n",
    "    for news in dict_feat:\n",
    "        probs = {}\n",
    "        vec = dict_feat[news]\n",
    "        for topic in words_topics:\n",
    "            probs[topic] = 0\n",
    "            w = words_topics[topic]\n",
    "            c = count_top[topic]\n",
    "            for i, word in enumerate(topic_words):\n",
    "                try:\n",
    "                    A = w[word][\"n_news\"]\n",
    "                    B = c\n",
    "                except:\n",
    "                    A = 1\n",
    "                    B = c\n",
    "                probs[topic] += log(A + 1) - log(B + M) if vec[i] else log(B + M - A - 1) - log(B + M)\n",
    "        guess[news] = max(probs.keys(), key=lambda x: probs[x])\n",
    "    return guess\n",
    "## EXEMPLE SORTIDA (LES XIFRES SÓN INVENTADES!):\n",
    "#Tòpic 1 encerts: 24 / Notícies: 34 / 70.59 %\n",
    "#Tòpic 2  encerts: 21 / Notícies: 26 / 80.77 %\n",
    "#Tòpic 3  encerts: 29 / Notícies: 32 / 90.62 %\n",
    "#Tòpic 4  encerts: 26 / Notícies: 29 / 89.66 %\n",
    "#\n",
    "#Error naive bayes: 17.36 %\n",
    "\n",
    "result = naive_bayes(data, dict_feat_vector)\n",
    "for key in result:\n",
    "    print result[key]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 8: </b> Indiqueu l'error de generalització fent servir *n-fold validation* (funció <b>n_fold ()</b> )\n",
    "\n",
    "**Aproximació a l'error de generalització fent servir  *n-fold validation* **.\n",
    "Una bona forma de veure com funcionaria el nostre classificador davant de dades sobre les quals no s'ha entrenat és fer servir l'estratègia n-fold. Aquesta estratègia testeja el classificador amb una partició de les dades d'entrenament i fa l'entrenament sobre la resta de dades que hem exclòs. Aquest procés d'exclusió es repeteix per cadascún dels *folds* de les dades d'entrenament. El nombre de *folds* determina quantes particions hem de fer, i per tant, les dades que hi ha en el conjut de test. Per exemple, en un *5-fold* validation, es fan 5 particions, les dades de test són un cinquè de les dades i l'entrenament es fa amb els quatre cinquens restants. El percentatge d'errors fent servir aquesta estratègia permet comparar classificadors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mètode per avaluar el classificador mitjançant la tècnica n-fold validation. n és el nombre de folds  \n",
    "#Ha de mostrar per pantalla el resultat obtingut \n",
    "def n_fold(df,feature_vector,n):\n",
    "     return 0\n",
    "    \n",
    "## EXEMPLE SORTIDA (LES XIFRES SÓN INVENTADES!):\n",
    "#Tòpic 1 encerts: 24 / Notícies: 34 / 70.59 %\n",
    "#Tòpic 2  encerts: 21 / Notícies: 26 / 80.77 %\n",
    "#Tòpic 3  encerts: 29 / Notícies: 32 / 90.62 %\n",
    "#Tòpic 4  encerts: 26 / Notícies: 29 / 89.66 %\n",
    "#\n",
    "\n",
    "#Error naive bayes: 17.36 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Resultat Final</b>\n",
    "\n",
    "Definició de la funció principal. Modifiqueu la funció per tal que s'ajusti a les vostres funcions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Main. Es criden a totes les funcions per a la correcte execució del programa.   \n",
    "def main():\n",
    "\n",
    "    import pandas as pd\n",
    "    data=pd.read_csv('./files/Boydstun_NYT_FrontPage_Dataset_1996-2006_0.csv')\n",
    "    N = 20 # Aquest parametre el podem canviar i fer proves per avaluar quin és el millor valor. \n",
    "    words_topics=count_words_topic(data,dicc_text)\n",
    "    top_words=topNwords(data,words_topics,N)\n",
    "    feature_vectors = create_features(data,top_words) \n",
    "    naive_bayes(data,feature_vectors) #fins aquí error d'entrenament. Fem servir totes les dades.\n",
    "    \n",
    "    \n",
    "    n_fold(data, feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
